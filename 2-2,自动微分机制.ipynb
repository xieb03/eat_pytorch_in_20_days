{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75b1c09e",
   "metadata": {},
   "source": [
    "# 2-2,自动微分机制"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b2a272",
   "metadata": {},
   "source": [
    "神经网络通常依赖反向传播求梯度来更新网络参数，求梯度过程通常是一件非常复杂而容易出错的事情。\n",
    "\n",
    "而深度学习框架可以帮助我们自动地完成这种求梯度运算。\n",
    "\n",
    "Pytorch一般通过反向传播 backward 方法 实现这种求梯度计算。该方法求得的梯度将存在对应自变量张量的grad属性下。\n",
    "\n",
    "**除此之外，也能够调用torch.autograd.grad 函数来实现求梯度计算，但用的比较少，一般用来求高阶导数。**\n",
    "\n",
    "这就是Pytorch的自动微分机制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "663ac7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.__version__ =  2.0.1+cu118\n",
      "torchvision.__version__ =  0.15.2+cu118\n",
      "pytorch_lightning.__version__ =  2.0.2\n",
      "torchtext.__version__ =  0.15.2\n",
      "torchdata.__version__ =  0.6.1\n",
      "torchmetrics.__version__ =  0.11.4\n",
      "torchkeras.__version__ =  3.8.2\n",
      "yaml.__version__ =  6.0\n",
      "tensorflow sed random seed fail.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import torchkeras\n",
    "\n",
    "#打印时间\n",
    "def printbar():\n",
    "    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(\"\\n\"+\"==========\"*8 + \"%s\"%nowtime)\n",
    "\n",
    "#mac系统上pytorch和matplotlib在jupyter中同时跑需要更改环境变量\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\" \n",
    "from python_cgtools.utils_date import *\n",
    "from python_cgtools.utils_torch import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e72ea02",
   "metadata": {},
   "source": [
    "```\n",
    "torch.__version__=1.10.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8ef2634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-29 00:42:39:start.........\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print_with_time(\"start.........\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80820ba",
   "metadata": {},
   "source": [
    "### 一，利用backward方法求导数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c211749b",
   "metadata": {},
   "source": [
    "backward 方法通常在一个标量张量上调用，该方法求得的梯度将存在对应自变量张量的grad属性下。\n",
    "\n",
    "如果调用的张量非标量，则要传入一个和它同形状 的gradient参数张量。\n",
    "\n",
    "相当于用该gradient参数张量与调用张量作向量点乘，得到的标量结果再反向传播。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43160576",
   "metadata": {},
   "source": [
    "**1, 标量的反向传播**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b528ce63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "\n",
    "# f(x) = a*x**2 + b*x + c的导数\n",
    "\n",
    "x = torch.tensor(0.0,requires_grad = True) # x需要被求导\n",
    "a = torch.tensor(1.0)\n",
    "b = torch.tensor(-2.0)\n",
    "c = torch.tensor(1.0)\n",
    "y = a*torch.pow(x,2) + b*x + c \n",
    "\n",
    "y.backward()\n",
    "dy_dx = x.grad\n",
    "# tensor(-2.)\n",
    "print(dy_dx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670b49a0",
   "metadata": {},
   "source": [
    "```\n",
    "tensor(-2.)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eee7aacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "\n",
    "x = torch.tensor([0.0, 0.0],requires_grad = True) # x需要被求导\n",
    "y = torch.pow(x,2)\n",
    "\n",
    "# RuntimeError: grad can be implicitly created only for scalar outputs\n",
    "# 求导默认只能对标量来求，一般模型的 loss 都是标量\n",
    "# y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc8f8a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "\n",
    "x = torch.tensor([0.0],requires_grad = False)\n",
    "y = torch.pow(x,2)\n",
    "\n",
    "# RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n",
    "# 能求导要求包含自变量，即 requires_grad=True 的量\n",
    "# y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09f3917",
   "metadata": {},
   "source": [
    "**2, 非标量的反向传播**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e08b055e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      " tensor([[0., 0.],\n",
      "        [1., 2.]], requires_grad=True)\n",
      "y:\n",
      " tensor([[1., 1.],\n",
      "        [0., 1.]], grad_fn=<AddBackward0>)\n",
      "x_grad:\n",
      " tensor([[-2., -2.],\n",
      "        [ 0.,  2.]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "\n",
    "# f(x) = a*x**2 + b*x + c\n",
    "\n",
    "x = torch.tensor([[0.0,0.0],[1.0,2.0]],requires_grad = True) # x需要被求导\n",
    "a = torch.tensor(1.0)\n",
    "b = torch.tensor(-2.0)\n",
    "c = torch.tensor(1.0)\n",
    "# [0, 0, 1, 4] - [0, 0, 2, 4] + 1 = [1, 1, 0, 1]\n",
    "y = a*torch.pow(x,2) + b*x + c \n",
    "\n",
    "gradient = torch.tensor([[1.0,1.0],[1.0,1.0]])\n",
    "\n",
    "# x:\n",
    "#  tensor([[0., 0.],\n",
    "#         [1., 2.]], requires_grad=True)\n",
    "# y:\n",
    "#  tensor([[1., 1.],\n",
    "#         [0., 1.]], grad_fn=<AddBackward0>)\n",
    "# x_grad:\n",
    "#  tensor([[-2., -2.],\n",
    "#         [ 0.,  2.]])\n",
    "print(\"x:\\n\",x)\n",
    "print(\"y:\\n\",y)\n",
    "y.backward(gradient = gradient)\n",
    "x_grad = x.grad\n",
    "print(\"x_grad:\\n\",x_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1edbaa3",
   "metadata": {},
   "source": [
    "```\n",
    "x:\n",
    " tensor([[0., 0.],\n",
    "        [1., 2.]], requires_grad=True)\n",
    "y:\n",
    " tensor([[1., 1.],\n",
    "        [0., 1.]], grad_fn=<AddBackward0>)\n",
    "x_grad:\n",
    " tensor([[-2., -2.],\n",
    "        [ 0.,  2.]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53911b0f",
   "metadata": {},
   "source": [
    "**3, 非标量的反向传播可以用标量的反向传播实现**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4323b96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[0., 0.],\n",
      "        [1., 2.]], requires_grad=True)\n",
      "y: tensor([[1., 1.],\n",
      "        [0., 1.]], grad_fn=<AddBackward0>)\n",
      "x_grad:\n",
      " tensor([[-2., -2.],\n",
      "        [ 0.,  2.]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "\n",
    "# f(x) = a*x**2 + b*x + c\n",
    "\n",
    "x = torch.tensor([[0.0,0.0],[1.0,2.0]],requires_grad = True) # x需要被求导\n",
    "a = torch.tensor(1.0)\n",
    "b = torch.tensor(-2.0)\n",
    "c = torch.tensor(1.0)\n",
    "y = a*torch.pow(x,2) + b*x + c \n",
    "\n",
    "gradient = torch.tensor([[1.0,1.0],[1.0,1.0]])\n",
    "# 将结果变成一个标量，如果 gradient 都是元素1，相当于将各个分量相加，对于此例来说，x 的分量之间没有耦合，因此结果等价于每个分量独立求导\n",
    "# 注意是偏导\n",
    "# z = (2a^2 - 2a + 1) + (2b^2 - 2b + 1) + (2c^2 - 2c + 1) + (2d^2 - 2d + 1)\n",
    "# ∂z/∂x_0 = 2a - 2 = -2\n",
    "# ∂z/∂x_1 = 2b - 2 = -2\n",
    "# ∂z/∂x_2 = 2c - 2 = 0\n",
    "# ∂z/∂x_3 = 2d - 2 = 2\n",
    "z = torch.sum(y*gradient)\n",
    "\n",
    "# x: tensor([[0., 0.],\n",
    "#         [1., 2.]], requires_grad=True)\n",
    "# y: tensor([[1., 1.],\n",
    "#         [0., 1.]], grad_fn=<AddBackward0>)\n",
    "# x_grad:\n",
    "#  tensor([[-2., -2.],\n",
    "#         [ 0.,  2.]])\n",
    "print(\"x:\",x)\n",
    "print(\"y:\",y)\n",
    "z.backward()\n",
    "x_grad = x.grad\n",
    "print(\"x_grad:\\n\",x_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d771601",
   "metadata": {},
   "source": [
    "```\n",
    "x: tensor([[0., 0.],\n",
    "        [1., 2.]], requires_grad=True)\n",
    "y: tensor([[1., 1.],\n",
    "        [0., 1.]], grad_fn=<AddBackward0>)\n",
    "x_grad:\n",
    " tensor([[-2., -2.],\n",
    "        [ 0.,  2.]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0b7f889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[0., 0.],\n",
      "        [1., 2.]], requires_grad=True)\n",
      "y: tensor([[0., 0.],\n",
      "        [2., 4.]], grad_fn=<MmBackward0>)\n",
      "x_grad:\n",
      " tensor([[1., 4.],\n",
      "        [2., 5.]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "\n",
    "# f(x) = a*x**2 + b*x + c\n",
    "\n",
    "x = torch.tensor([[0.0,0.0],[1.0,2.0]],requires_grad = True) # x需要被求导\n",
    "y = torch.matmul(x, x)\n",
    "\n",
    "gradient = torch.tensor([[1.0,1.0],[1.0,1.0]])\n",
    "# 如果标量对于 x 的分量并不独立，即分量之间有交叉，那么结果会更复杂一些\n",
    "# z = [[a, b], [c, d]] * [[a, b], [c, d]] = (a^2 + bc) + (ab + bd) + (ca + dc) + (cb + d^2) = a^2 + ab + ac + 2bc + bd + cd + d^2\n",
    "# a = 0, b = 0, c = 1, d = 2\n",
    "# ∂z/∂a = 2a + b + c = 1\n",
    "# ∂z/∂b = a + 2c + d = 4\n",
    "# ∂z/∂c = a + 2b + d = 2\n",
    "# ∂z/∂d = b + c + 2d = 5\n",
    "z = torch.sum(y*gradient)\n",
    "\n",
    "# x: tensor([[0., 0.],\n",
    "#         [1., 2.]], requires_grad=True)\n",
    "# y: tensor([[0., 0.],\n",
    "#         [2., 4.]], grad_fn=<MmBackward0>)\n",
    "# x_grad:\n",
    "#  tensor([[1., 4.],\n",
    "#         [2., 5.]])\n",
    "print(\"x:\",x)\n",
    "print(\"y:\",y)\n",
    "z.backward()\n",
    "x_grad = x.grad\n",
    "print(\"x_grad:\\n\",x_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb1d1421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[0., 0.],\n",
      "        [1., 2.]], requires_grad=True)\n",
      "y: tensor([[0., 0.],\n",
      "        [2., 4.]], grad_fn=<MmBackward0>)\n",
      "x_grad:\n",
      " tensor([[ 2.,  8.],\n",
      "        [ 4., 10.]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "\n",
    "# f(x) = a*x**2 + b*x + c\n",
    "\n",
    "x = torch.tensor([[0.0,0.0],[1.0,2.0]],requires_grad = True) # x需要被求导\n",
    "y = torch.matmul(x, x)\n",
    "\n",
    "gradient = torch.tensor([[1.0,1.0],[1.0,1.0]])\n",
    "# 如果标量对于 x 的分量并不独立，即分量之间有交叉，那么结果会更复杂一些\n",
    "# z = [[a, b], [c, d]] * [[a, b], [c, d]] = (a^2 + bc) + (ab + bd) + (ca + dc) + (cb + d^2) = a^2 + ab + ac + 2bc + bd + cd + d^2\n",
    "# a = 0, b = 0, c = 1, d = 2\n",
    "# ∂z/∂a = 2a + b + c = 1\n",
    "# ∂z/∂b = a + 2c + d = 4\n",
    "# ∂z/∂c = a + 2b + d = 2\n",
    "# ∂z/∂d = b + c + 2d = 5\n",
    "z = torch.sum(y*gradient)\n",
    "\n",
    "# x: tensor([[0., 0.],\n",
    "#         [1., 2.]], requires_grad=True)\n",
    "# y: tensor([[0., 0.],\n",
    "#         [2., 4.]], grad_fn=<MmBackward0>)\n",
    "# x_grad:\n",
    "#  tensor([[ 2.,  8.],\n",
    "#         [ 4., 10.]])\n",
    "print(\"x:\",x)\n",
    "print(\"y:\",y)\n",
    "# 如果要多次求梯度，首先要把 retain_graph 设置成 True，然后 pytorch 默认会把多次求梯度的结果加和\n",
    "# 对于此例可以看到梯度结果乘以了 2\n",
    "z.backward(retain_graph=True)\n",
    "z.backward()\n",
    "x_grad = x.grad\n",
    "print(\"x_grad:\\n\",x_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad40b01",
   "metadata": {},
   "source": [
    "### 二，利用autograd.grad方法求导数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53328d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.)\n",
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "\n",
    "# f(x) = a*x**2 + b*x + c的导数\n",
    "\n",
    "x = torch.tensor(0.0,requires_grad = True) # x需要被求导\n",
    "a = torch.tensor(1.0)\n",
    "b = torch.tensor(-2.0)\n",
    "c = torch.tensor(1.0)\n",
    "y = a*torch.pow(x,2) + b*x + c\n",
    "\n",
    "\n",
    "# create_graph 设置为 True 将允许创建更高阶的导数 \n",
    "# dy / dx = 2x + b = -2\n",
    "dy_dx = torch.autograd.grad(y,x,create_graph=True)[0]\n",
    "# tensor(-2.)\n",
    "print(dy_dx.data)\n",
    "\n",
    "# 求二阶导数\n",
    "# dy2/dx2 = d(2x +b) / dx = 2\n",
    "dy2_dx2 = torch.autograd.grad(dy_dx,x)[0] \n",
    "# tensor(2.)\n",
    "print(dy2_dx2.data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a93cf05",
   "metadata": {},
   "source": [
    "```\n",
    "tensor(-2.)\n",
    "tensor(2.)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "320ddde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.) tensor(1.)\n",
      "tensor(3.) tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "\n",
    "x1 = torch.tensor(1.0,requires_grad = True) # x需要被求导\n",
    "x2 = torch.tensor(2.0,requires_grad = True)\n",
    "\n",
    "y1 = x1*x2\n",
    "y2 = x1+x2\n",
    "\n",
    "\n",
    "# 允许同时对多个自变量求导数\n",
    "# dy1/dx1 = x2 = 2\n",
    "# dy1/dx2 = 1\n",
    "# 必须设置 retain_graph = True,否则\n",
    "# Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). \n",
    "# Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). \n",
    "# Specify retain_graph=True if you need to backward through the graph a second time \n",
    "# or if you need to access saved tensors after calling backward.\n",
    "(dy1_dx1,dy1_dx2) = torch.autograd.grad(outputs=y1,inputs = [x1,x2],retain_graph = True)\n",
    "# tensor(2.) tensor(1.)\n",
    "print(dy1_dx1,dy1_dx2)\n",
    "\n",
    "# 如果有多个因变量，相当于把多个因变量的梯度结果求和\n",
    "# dy1/dx1 + dy2/dx1 = x2 + 1 = 3\n",
    "# dy1/dx2 + dy2/dx2 = x1 + 1 = 2\n",
    "(dy12_dx1,dy12_dx2) = torch.autograd.grad(outputs=[y1,y2],inputs = [x1,x2])\n",
    "print(dy12_dx1,dy12_dx2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c5a169",
   "metadata": {},
   "source": [
    "```\n",
    "tensor(2.) tensor(1.)\n",
    "tensor(3.) tensor(2.)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8600f6c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6169370f",
   "metadata": {},
   "source": [
    "### 三，利用自动微分和优化器求极值（不一定是最）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31648fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= tensor(0.) ; x= tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "\n",
    "# f(x) = a*x**2 + b*x + c的最小值\n",
    "\n",
    "x = torch.tensor(0.0,requires_grad = True) # x需要被求导\n",
    "a = torch.tensor(1.0)\n",
    "b = torch.tensor(-2.0)\n",
    "c = torch.tensor(1.0)\n",
    "\n",
    "optimizer = torch.optim.SGD(params=[x],lr = 0.01)\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    result = a*torch.pow(x,2) + b*x + c \n",
    "    return(result)\n",
    "\n",
    "# 其实就是模拟了梯度下降，注意 y = f(x) 每次都会用新的 x 重新计算，相当于每个 batch 结束后，要更新输入，重新算梯度\n",
    "x_list = list()\n",
    "y_list = list()\n",
    "grad_list = list()\n",
    "for i in range(500):\n",
    "    optimizer.zero_grad()\n",
    "    x_list.append(x.item())\n",
    "    y = f(x)\n",
    "    y.backward()\n",
    "    optimizer.step()\n",
    "#     要放在 optimizer.step() 前面，否则 x 的值会被梯度下降更新\n",
    "#     x_list.append(x.item())\n",
    "    y_list.append(y.item())\n",
    "    grad_list.append(x.grad.item())\n",
    "    \n",
    "print(\"y=\",f(x).data,\";\",\"x=\",x.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5cc5fd",
   "metadata": {},
   "source": [
    "```\n",
    "y= tensor(0.) ; x= tensor(1.0000)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bcb0cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>grad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.960400</td>\n",
       "      <td>-1.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.039600</td>\n",
       "      <td>0.922368</td>\n",
       "      <td>-1.920800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.058808</td>\n",
       "      <td>0.885842</td>\n",
       "      <td>-1.882384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.077632</td>\n",
       "      <td>0.850763</td>\n",
       "      <td>-1.844736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0.999955</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.999956</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.999956</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0.999957</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.999958</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            x         y      grad\n",
       "0    0.000000  1.000000 -2.000000\n",
       "1    0.020000  0.960400 -1.960000\n",
       "2    0.039600  0.922368 -1.920800\n",
       "3    0.058808  0.885842 -1.882384\n",
       "4    0.077632  0.850763 -1.844736\n",
       "..        ...       ...       ...\n",
       "495  0.999955  0.000000 -0.000091\n",
       "496  0.999956  0.000000 -0.000089\n",
       "497  0.999956  0.000000 -0.000087\n",
       "498  0.999957  0.000000 -0.000085\n",
       "499  0.999958  0.000000 -0.000084\n",
       "\n",
       "[500 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x\ty\tgrad\n",
    "# 0\t0.020000\t1.000000\t-2.000000\n",
    "# 1\t0.039600\t0.960400\t-1.960000\n",
    "# 2\t0.058808\t0.922368\t-1.920800\n",
    "# 3\t0.077632\t0.885842\t-1.882384\n",
    "# 4\t0.096079\t0.850763\t-1.844736\n",
    "# ...\t...\t...\t...\n",
    "# 495\t0.999956\t0.000000\t-0.000091\n",
    "# 496\t0.999956\t0.000000\t-0.000089\n",
    "# 497\t0.999957\t0.000000\t-0.000087\n",
    "# 498\t0.999958\t0.000000\t-0.000085\n",
    "# 499\t0.999959\t0.000000\t-0.000084\n",
    "df = pd.DataFrame(zip(x_list, y_list, grad_list), columns=[\"x\", \"y\", \"grad\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2078a097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2df7506",
   "metadata": {},
   "source": [
    "**如果本书对你有所帮助，想鼓励一下作者，记得给本项目加一颗星星star⭐️，并分享给你的朋友们喔😊!** \n",
    "\n",
    "如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号\"算法美食屋\"下留言。作者时间和精力有限，会酌情予以回复。\n",
    "\n",
    "也可以在公众号后台回复关键字：**加群**，加入读者交流群和大家讨论。\n",
    "\n",
    "![算法美食屋logo.png](https://tva1.sinaimg.cn/large/e6c9d24egy1h41m2zugguj20k00b9q46.jpg)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,md",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
