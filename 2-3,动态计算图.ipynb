{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8077c48",
   "metadata": {},
   "source": [
    "# 2-3,åŠ¨æ€è®¡ç®—å›¾"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a500f1",
   "metadata": {},
   "source": [
    "æœ¬èŠ‚æˆ‘ä»¬å°†ä»‹ç» Pytorchçš„åŠ¨æ€è®¡ç®—å›¾ã€‚\n",
    "\n",
    "åŒ…æ‹¬ï¼š \n",
    "\n",
    "* åŠ¨æ€è®¡ç®—å›¾ç®€ä»‹\n",
    "\n",
    "* è®¡ç®—å›¾ä¸­çš„Function\n",
    "\n",
    "* è®¡ç®—å›¾å’Œåå‘ä¼ æ’­\n",
    "\n",
    "* å¶å­èŠ‚ç‚¹å’Œéå¶å­èŠ‚ç‚¹\n",
    "\n",
    "* è®¡ç®—å›¾åœ¨TensorBoardä¸­çš„å¯è§†åŒ–\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cefbb31",
   "metadata": {},
   "source": [
    "### ä¸€ï¼ŒåŠ¨æ€è®¡ç®—å›¾ç®€ä»‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e9b86b",
   "metadata": {},
   "source": [
    "![](./data/torchåŠ¨æ€å›¾.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70c6ecf",
   "metadata": {},
   "source": [
    "Pytorchçš„è®¡ç®—å›¾ç”±èŠ‚ç‚¹å’Œè¾¹ç»„æˆï¼ŒèŠ‚ç‚¹è¡¨ç¤ºå¼ é‡æˆ–è€…Functionï¼Œè¾¹è¡¨ç¤ºå¼ é‡å’ŒFunctionä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚\n",
    "\n",
    "Pytorchä¸­çš„è®¡ç®—å›¾æ˜¯åŠ¨æ€å›¾ã€‚è¿™é‡Œçš„åŠ¨æ€ä¸»è¦æœ‰ä¸¤é‡å«ä¹‰ã€‚\n",
    "\n",
    "**ç¬¬ä¸€å±‚å«ä¹‰æ˜¯ï¼šè®¡ç®—å›¾çš„æ­£å‘ä¼ æ’­æ˜¯ç«‹å³æ‰§è¡Œçš„ã€‚æ— éœ€ç­‰å¾…å®Œæ•´çš„è®¡ç®—å›¾åˆ›å»ºå®Œæ¯•ï¼Œæ¯æ¡è¯­å¥éƒ½ä¼šåœ¨è®¡ç®—å›¾ä¸­åŠ¨æ€æ·»åŠ èŠ‚ç‚¹å’Œè¾¹ï¼Œå¹¶ç«‹å³æ‰§è¡Œæ­£å‘ä¼ æ’­å¾—åˆ°è®¡ç®—ç»“æœã€‚**\n",
    "\n",
    "**ç¬¬äºŒå±‚å«ä¹‰æ˜¯ï¼šè®¡ç®—å›¾åœ¨åå‘ä¼ æ’­åç«‹å³é”€æ¯ã€‚ä¸‹æ¬¡è°ƒç”¨éœ€è¦é‡æ–°æ„å»ºè®¡ç®—å›¾ã€‚å¦‚æœåœ¨ç¨‹åºä¸­ä½¿ç”¨äº†backwardæ–¹æ³•æ‰§è¡Œäº†åå‘ä¼ æ’­ï¼Œæˆ–è€…åˆ©ç”¨torch.autograd.gradæ–¹æ³•è®¡ç®—äº†æ¢¯åº¦ï¼Œé‚£ä¹ˆåˆ›å»ºçš„è®¡ç®—å›¾ä¼šè¢«ç«‹å³é”€æ¯ï¼Œé‡Šæ”¾å­˜å‚¨ç©ºé—´ï¼Œä¸‹æ¬¡è°ƒç”¨éœ€è¦é‡æ–°åˆ›å»ºã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆä¸Šä¸€ç« è¦ç”¨ retain_graph = True çš„åŸå› ï¼Œç®—å®Œä¸€æ¬¡ä¹‹åï¼ŒåŸæœ‰çš„è®¡ç®—å›¾å·²ç»ä¸å­˜åœ¨äº†ï¼Œç›¸å½“äºå·²ç»æŠŠå…¬å¼å¿˜è®°äº†**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5c20489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.__version__ =  2.0.1+cu118\n",
      "tensorboard.__version__ =  2.13.0\n",
      "torchvision.__version__ =  0.15.2+cu118\n",
      "pytorch_lightning.__version__ =  2.0.2\n",
      "torchtext.__version__ =  0.15.2\n",
      "torchdata.__version__ =  0.6.1\n",
      "torchmetrics.__version__ =  0.11.4\n",
      "torchkeras.__version__ =  3.8.2\n",
      "yaml.__version__ =  6.0\n",
      "tensorflow sed random seed fail.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import torchkeras\n",
    "\n",
    "#æ‰“å°æ—¶é—´\n",
    "def printbar():\n",
    "    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(\"\\n\"+\"==========\"*8 + \"%s\"%nowtime)\n",
    "\n",
    "#macç³»ç»Ÿä¸Špytorchå’Œmatplotlibåœ¨jupyterä¸­åŒæ—¶è·‘éœ€è¦æ›´æ”¹ç¯å¢ƒå˜é‡\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\" \n",
    "from python_cgtools.utils_date import *\n",
    "from python_cgtools.utils_torch import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9153578b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-29 12:32:04:start.........\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print_with_time(\"start.........\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1707038",
   "metadata": {},
   "source": [
    "**1ï¼Œè®¡ç®—å›¾çš„æ­£å‘ä¼ æ’­æ˜¯ç«‹å³æ‰§è¡Œçš„ã€‚**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19d73e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.7748)\n",
      "tensor([[ 4.6815],\n",
      "        [ 7.5891],\n",
      "        [-4.4283],\n",
      "        [ 6.0147],\n",
      "        [-3.0943],\n",
      "        [ 5.8340],\n",
      "        [ 6.5043],\n",
      "        [ 5.6292],\n",
      "        [-3.0152],\n",
      "        [ 4.2290]])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "w = torch.tensor([[3.0,1.0]],requires_grad=True)\n",
    "b = torch.tensor([[3.0]],requires_grad=True)\n",
    "X = torch.randn(10,2)\n",
    "Y = torch.randn(10,1)\n",
    "Y_hat = X@w.t() + b  # Y_hatå®šä¹‰åå…¶æ­£å‘ä¼ æ’­è¢«ç«‹å³æ‰§è¡Œï¼Œä¸å…¶åé¢çš„lossåˆ›å»ºè¯­å¥æ— å…³\n",
    "loss = torch.mean(torch.pow(Y_hat-Y,2))\n",
    "\n",
    "print(loss.data)\n",
    "print(Y_hat.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f92804c",
   "metadata": {},
   "source": [
    "```\n",
    "tensor(17.8969)\n",
    "tensor([[3.2613],\n",
    "        [4.7322],\n",
    "        [4.5037],\n",
    "        [7.5899],\n",
    "        [7.0973],\n",
    "        [1.3287],\n",
    "        [6.1473],\n",
    "        [1.3492],\n",
    "        [1.3911],\n",
    "        [1.2150]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8fd5db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "adbd5f4b",
   "metadata": {},
   "source": [
    "**2ï¼Œè®¡ç®—å›¾åœ¨åå‘ä¼ æ’­åç«‹å³é”€æ¯ã€‚**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8506281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "w = torch.tensor([[3.0,1.0]],requires_grad=True)\n",
    "b = torch.tensor([[3.0]],requires_grad=True)\n",
    "X = torch.randn(10,2)\n",
    "Y = torch.randn(10,1)\n",
    "Y_hat = X@w.t() + b  # Y_hatå®šä¹‰åå…¶æ­£å‘ä¼ æ’­è¢«ç«‹å³æ‰§è¡Œï¼Œä¸å…¶åé¢çš„lossåˆ›å»ºè¯­å¥æ— å…³\n",
    "loss = torch.mean(torch.pow(Y_hat-Y,2))\n",
    "\n",
    "#è®¡ç®—å›¾åœ¨åå‘ä¼ æ’­åç«‹å³é”€æ¯ï¼Œå¦‚æœéœ€è¦ä¿ç•™è®¡ç®—å›¾, éœ€è¦è®¾ç½®retain_graph = True\n",
    "loss.backward()  #loss.backward(retain_graph = True) \n",
    "\n",
    "#loss.backward() #å¦‚æœå†æ¬¡æ‰§è¡Œåå‘ä¼ æ’­å°†æŠ¥é”™\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5561cef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d2dbb7f",
   "metadata": {},
   "source": [
    "### äºŒï¼Œè®¡ç®—å›¾ä¸­çš„Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7707accc",
   "metadata": {},
   "source": [
    "è®¡ç®—å›¾ä¸­çš„ å¼ é‡æˆ‘ä»¬å·²ç»æ¯”è¾ƒç†Ÿæ‚‰äº†, è®¡ç®—å›¾ä¸­çš„å¦å¤–ä¸€ç§èŠ‚ç‚¹æ˜¯Function, å®é™…ä¸Šå°±æ˜¯ Pytorchä¸­å„ç§å¯¹å¼ é‡æ“ä½œçš„å‡½æ•°ã€‚\n",
    "\n",
    "è¿™äº›Functionå’Œæˆ‘ä»¬Pythonä¸­çš„å‡½æ•°æœ‰ä¸€ä¸ªè¾ƒå¤§çš„åŒºåˆ«ï¼Œé‚£å°±æ˜¯å®ƒåŒæ—¶åŒ…æ‹¬æ­£å‘è®¡ç®—é€»è¾‘å’Œåå‘ä¼ æ’­çš„é€»è¾‘ã€‚\n",
    "\n",
    "æˆ‘ä»¬å¯ä»¥é€šè¿‡ç»§æ‰¿torch.autograd.Functionæ¥åˆ›å»ºè¿™ç§æ”¯æŒåå‘ä¼ æ’­çš„Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbc5a73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grad_output æ˜¯ä¸€ç›´ä»æœ€ååå‘ä¼ è¿‡æ¥çš„å¯¼æ•°\n",
    "# y = relu(x)\n",
    "# grad_output = dloss / dy\n",
    "# grad_input = dloss / dx = (dloss / dy) * (dy / dx) = grad_outpout * d relu(x) / dx\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "   \n",
    "    #æ­£å‘ä¼ æ’­é€»è¾‘ï¼Œå¯ä»¥ç”¨ctxå­˜å‚¨ä¸€äº›å€¼ï¼Œä¾›åå‘ä¼ æ’­ä½¿ç”¨ã€‚\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    #åå‘ä¼ æ’­é€»è¾‘\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7624f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000]])\n",
      "tensor([[4.5000]])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "w = torch.tensor([[3.0,1.0]],requires_grad=True)\n",
    "b = torch.tensor([[3.0]],requires_grad=True)\n",
    "X = torch.tensor([[-1.0,-1.0],[1.0,1.0]])\n",
    "Y = torch.tensor([[2.0,3.0]])\n",
    "\n",
    "# reluç°åœ¨ä¹Ÿå¯ä»¥å…·æœ‰æ­£å‘ä¼ æ’­å’Œåå‘ä¼ æ’­åŠŸèƒ½\n",
    "relu = MyReLU.apply\n",
    "Y_hat = relu(X@w.t() + b)\n",
    "loss = torch.mean(torch.pow(Y_hat-Y,2))\n",
    "\n",
    "# dloss / dy_hat = 2(y_hat - y)\n",
    "# y_hat = 2(wx + b - y)\n",
    "# dloss / db = (dloss / dy_hat) * (dy_hat / dw) = 2(y_hat - y) * 2\n",
    "# dloss / dw = (dloss / dy_hat) * (dy_hat / dw) = 2(y_hat - y) * 2x\n",
    "loss.backward()\n",
    "\n",
    "# tensor([[4.5000, 4.5000]])\n",
    "# tensor([[4.5000]])\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d557de",
   "metadata": {},
   "source": [
    "```\n",
    "tensor([[4.5000, 4.5000]])\n",
    "tensor([[4.5000]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5570da3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.autograd.function.MyReLUBackward object at 0x00000227FC3D4900>\n"
     ]
    }
   ],
   "source": [
    "# Y_hatçš„æ¢¯åº¦å‡½æ•°å³æ˜¯æˆ‘ä»¬è‡ªå·±æ‰€å®šä¹‰çš„ MyReLU.backward\n",
    "# <torch.autograd.function.MyReLUBackward object at 0x000001EE7D663900>\n",
    "print(Y_hat.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbe47c6",
   "metadata": {},
   "source": [
    "```\n",
    "<torch.autograd.function.MyReLUBackward object at 0x1205a46c8>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01c3f73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-f48dd163d866>:3: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:491.)\n",
      "  print(Y_hat.grad)\n"
     ]
    }
   ],
   "source": [
    "# UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed\n",
    "# None\n",
    "print(Y_hat.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6753209e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.) tensor(-4.)\n"
     ]
    }
   ],
   "source": [
    "# å¤šå…ƒå‡½æ•°ç¤ºèŒƒ\n",
    "\n",
    "class MyMinus(torch.autograd.Function):\n",
    "   \n",
    "    #æ­£å‘ä¼ æ’­é€»è¾‘ï¼Œå¯ä»¥ç”¨ctxå­˜å‚¨ä¸€äº›å€¼ï¼Œä¾›åå‘ä¼ æ’­ä½¿ç”¨ã€‚\n",
    "    @staticmethod\n",
    "    def forward(ctx, a, b):\n",
    "        ctx.save_for_backward(a, b)\n",
    "        return a - b\n",
    "\n",
    "    #åå‘ä¼ æ’­é€»è¾‘\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        a, b = ctx.saved_tensors\n",
    "        grad_a, grad_b = grad_output.clone(), -grad_output.clone()\n",
    "        return grad_a, grad_b\n",
    "    \n",
    "a = torch.tensor(3.0,requires_grad=True)\n",
    "b = torch.tensor(1.0,requires_grad=True)\n",
    "minus = MyMinus.apply\n",
    "loss = minus(a, b)**2\n",
    "loss.backward()\n",
    "# tensor(4.) tensor(-4.)\n",
    "print(a.grad, b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d70ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74879f4c",
   "metadata": {},
   "source": [
    "### ä¸‰ï¼Œè®¡ç®—å›¾ä¸åå‘ä¼ æ’­"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cd30c3",
   "metadata": {},
   "source": [
    "äº†è§£äº†Functionçš„åŠŸèƒ½ï¼Œæˆ‘ä»¬å¯ä»¥ç®€å•åœ°ç†è§£ä¸€ä¸‹åå‘ä¼ æ’­çš„åŸç†å’Œè¿‡ç¨‹ã€‚ç†è§£è¯¥éƒ¨åˆ†åŸç†éœ€è¦ä¸€äº›é«˜ç­‰æ•°å­¦ä¸­æ±‚å¯¼é“¾å¼æ³•åˆ™çš„åŸºç¡€çŸ¥è¯†ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33988d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "x = torch.tensor(3.0,requires_grad=True)\n",
    "y1 = x + 1\n",
    "y2 = 2*x\n",
    "loss = (y1-y2)**2\n",
    "\n",
    "loss.backward()\n",
    "# tensor(4.)\n",
    "# dloss / dx = (dloss / dy1 * dy1 / dx) + (dloss / dy2 * dy2 / dx)\n",
    "#            = (2(y1 - y2) * 1) + (2(y2 - y1) * 2)\n",
    "#            = 2 * -2 * 1 + 2 * 2 * 2\n",
    "#            = -4 + 8\n",
    "#            = 4\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1fc3d8",
   "metadata": {},
   "source": [
    "loss.backward()è¯­å¥è°ƒç”¨åï¼Œä¾æ¬¡å‘ç”Ÿä»¥ä¸‹è®¡ç®—è¿‡ç¨‹ã€‚\n",
    "\n",
    "1ï¼Œlossè‡ªå·±çš„gradæ¢¯åº¦èµ‹å€¼ä¸º1ï¼Œå³å¯¹è‡ªèº«çš„æ¢¯åº¦ä¸º1ã€‚\n",
    "\n",
    "2ï¼Œlossæ ¹æ®å…¶è‡ªèº«æ¢¯åº¦ä»¥åŠå…³è”çš„backwardæ–¹æ³•ï¼Œè®¡ç®—å‡ºå…¶å¯¹åº”çš„è‡ªå˜é‡å³y1å’Œy2çš„æ¢¯åº¦ï¼Œå°†è¯¥å€¼èµ‹å€¼åˆ°y1.gradå’Œy2.gradã€‚\n",
    "\n",
    "**3ï¼Œy2å’Œy1æ ¹æ®å…¶è‡ªèº«æ¢¯åº¦ä»¥åŠå…³è”çš„backwardæ–¹æ³•, åˆ†åˆ«è®¡ç®—å‡ºå…¶å¯¹åº”çš„è‡ªå˜é‡xçš„æ¢¯åº¦ï¼Œx.gradå°†å…¶æ”¶åˆ°çš„å¤šä¸ªæ¢¯åº¦å€¼ç´¯åŠ ã€‚**\n",
    "\n",
    "ï¼ˆæ³¨æ„ï¼Œ1,2,3æ­¥éª¤çš„æ±‚æ¢¯åº¦é¡ºåºå’Œå¯¹å¤šä¸ªæ¢¯åº¦å€¼çš„ç´¯åŠ è§„åˆ™æ°å¥½æ˜¯æ±‚å¯¼é“¾å¼æ³•åˆ™çš„ç¨‹åºè¡¨è¿°ï¼‰\n",
    "\n",
    "æ­£å› ä¸ºæ±‚å¯¼é“¾å¼æ³•åˆ™è¡ç”Ÿçš„æ¢¯åº¦ç´¯åŠ è§„åˆ™ï¼Œå¼ é‡çš„gradæ¢¯åº¦ä¸ä¼šè‡ªåŠ¨æ¸…é›¶ï¼Œåœ¨éœ€è¦çš„æ—¶å€™éœ€è¦æ‰‹åŠ¨ç½®é›¶ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d86676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c20cd66d",
   "metadata": {},
   "source": [
    "### å››ï¼Œå¶å­èŠ‚ç‚¹å’Œéå¶å­èŠ‚ç‚¹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db4c2fc",
   "metadata": {},
   "source": [
    "æ‰§è¡Œä¸‹é¢ä»£ç ï¼Œæˆ‘ä»¬ä¼šå‘ç° loss.gradå¹¶ä¸æ˜¯æˆ‘ä»¬æœŸæœ›çš„1,è€Œæ˜¯ Noneã€‚\n",
    "\n",
    "ç±»ä¼¼åœ° y1.grad ä»¥åŠ y2.gradä¹Ÿæ˜¯ None.\n",
    "\n",
    "è¿™æ˜¯ä¸ºä»€ä¹ˆå‘¢ï¼Ÿè¿™æ˜¯ç”±äºå®ƒä»¬ä¸æ˜¯å¶å­èŠ‚ç‚¹å¼ é‡ã€‚\n",
    "\n",
    "**åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œåªæœ‰ is_leaf=True çš„å¶å­èŠ‚ç‚¹ï¼Œéœ€è¦æ±‚å¯¼çš„å¼ é‡çš„å¯¼æ•°ç»“æœæ‰ä¼šè¢«æœ€åä¿ç•™ä¸‹æ¥ã€‚\n",
    "é‚£ä¹ˆä»€ä¹ˆæ˜¯å¶å­èŠ‚ç‚¹å¼ é‡å‘¢ï¼Ÿå¶å­èŠ‚ç‚¹å¼ é‡éœ€è¦æ»¡è¶³ä¸¤ä¸ªæ¡ä»¶ã€‚\n",
    "1ï¼Œå¶å­èŠ‚ç‚¹å¼ é‡æ˜¯ç”±ç”¨æˆ·ç›´æ¥åˆ›å»ºçš„å¼ é‡ï¼Œè€Œéç”±æŸä¸ªFunctioné€šè¿‡è®¡ç®—å¾—åˆ°çš„å¼ é‡ã€‚\n",
    "2ï¼Œå¶å­èŠ‚ç‚¹å¼ é‡çš„ requires_gradå±æ€§å¿…é¡»ä¸ºTrue.**\n",
    "\n",
    "Pytorchè®¾è®¡è¿™æ ·çš„è§„åˆ™ä¸»è¦æ˜¯ä¸ºäº†èŠ‚çº¦å†…å­˜æˆ–è€…æ˜¾å­˜ç©ºé—´ï¼Œå› ä¸ºå‡ ä¹æ‰€æœ‰çš„æ—¶å€™ï¼Œç”¨æˆ·åªä¼šå…³å¿ƒä»–è‡ªå·±ç›´æ¥åˆ›å»ºçš„å¼ é‡çš„æ¢¯åº¦ã€‚\n",
    "\n",
    "**æ‰€æœ‰ä¾èµ–äºå¶å­èŠ‚ç‚¹å¼ é‡çš„å¼ é‡, å…¶requires_grad å±æ€§å¿…å®šæ˜¯Trueçš„ï¼Œä½†å…¶æ¢¯åº¦å€¼åªåœ¨è®¡ç®—è¿‡ç¨‹ä¸­è¢«ç”¨åˆ°ï¼Œä¸ä¼šæœ€ç»ˆå­˜å‚¨åˆ°gradå±æ€§ä¸­ã€‚**\n",
    "\n",
    "å¦‚æœéœ€è¦ä¿ç•™ä¸­é—´è®¡ç®—ç»“æœçš„æ¢¯åº¦åˆ°gradå±æ€§ä¸­ï¼Œå¯ä»¥ä½¿ç”¨ retain_gradæ–¹æ³•ã€‚\n",
    "å¦‚æœä»…ä»…æ˜¯ä¸ºäº†è°ƒè¯•ä»£ç æŸ¥çœ‹æ¢¯åº¦å€¼ï¼Œå¯ä»¥åˆ©ç”¨register_hookæ‰“å°æ—¥å¿—ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fb81bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.grad: None\n",
      "y1.grad: None\n",
      "y2.grad: None\n",
      "tensor(4.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-77fbc144ec27>:13: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:491.)\n",
      "  print(\"loss.grad:\", loss.grad)\n",
      "<ipython-input-11-77fbc144ec27>:14: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:491.)\n",
      "  print(\"y1.grad:\", y1.grad)\n",
      "<ipython-input-11-77fbc144ec27>:15: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:491.)\n",
      "  print(\"y2.grad:\", y2.grad)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "x = torch.tensor(3.0,requires_grad=True)\n",
    "y1 = x + 1\n",
    "y2 = 2*x\n",
    "loss = (y1-y2)**2\n",
    "\n",
    "loss.backward()\n",
    "# loss.grad: None\n",
    "# y1.grad: None\n",
    "# y2.grad: None\n",
    "# tensor(4.)\n",
    "print(\"loss.grad:\", loss.grad)\n",
    "print(\"y1.grad:\", y1.grad)\n",
    "print(\"y2.grad:\", y2.grad)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1542934b",
   "metadata": {},
   "source": [
    "```\n",
    "loss.grad: None\n",
    "y1.grad: None\n",
    "y2.grad: None\n",
    "tensor(4.)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e253648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# True\n",
    "# False\n",
    "# False\n",
    "# False\n",
    "print(x.is_leaf)\n",
    "print(y1.is_leaf)\n",
    "print(y2.is_leaf)\n",
    "print(loss.is_leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7670308b",
   "metadata": {},
   "source": [
    "```\n",
    "True\n",
    "False\n",
    "False\n",
    "False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207753e2",
   "metadata": {},
   "source": [
    "åˆ©ç”¨retain_gradå¯ä»¥ä¿ç•™éå¶å­èŠ‚ç‚¹çš„æ¢¯åº¦å€¼ï¼Œåˆ©ç”¨register_hookå¯ä»¥æŸ¥çœ‹éå¶å­èŠ‚ç‚¹çš„æ¢¯åº¦å€¼ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22d9401c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y2 grad:  tensor(4.)\n",
      "y1 grad:  tensor(-4.)\n",
      "11111\n",
      "loss.grad: tensor(1.)\n",
      "y.grad: None\n",
      "x.grad: tensor(4.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-7936a861ceb3>:26: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:491.)\n",
      "  print(\"y.grad:\", y1.grad)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "#æ­£å‘ä¼ æ’­\n",
    "x = torch.tensor(3.0,requires_grad=True)\n",
    "y1 = x + 1\n",
    "y2 = 2*x\n",
    "loss = (y1-y2)**2\n",
    "\n",
    "#éå¶å­èŠ‚ç‚¹æ¢¯åº¦æ˜¾ç¤ºæ§åˆ¶\n",
    "y1.register_hook(lambda grad: print('y1 grad: ', grad))\n",
    "y2.register_hook(lambda grad: print('y2 grad: ', grad))\n",
    "loss.retain_grad()\n",
    "\n",
    "#åå‘ä¼ æ’­\n",
    "loss.backward()\n",
    "\n",
    "# y2 grad:  tensor(4.)\n",
    "# y1 grad:  tensor(-4.)\n",
    "# æ³¨æ„ä¸Šè¿°ä»£ç æ˜¯åœ¨ loss.backward() æ‰“å°å‡ºæ¥çš„\n",
    "# 11111\n",
    "# loss.grad: tensor(1.)\n",
    "# y.grad: None\n",
    "# x.grad: tensor(4.)\n",
    "print(11111)\n",
    "print(\"loss.grad:\", loss.grad)\n",
    "print(\"y.grad:\", y1.grad)\n",
    "print(\"x.grad:\", x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91887ca8",
   "metadata": {},
   "source": [
    "```\n",
    "y2 grad:  tensor(4.)\n",
    "y1 grad:  tensor(-4.)\n",
    "loss.grad: tensor(1.)\n",
    "x.grad: tensor(4.)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3110a64c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e11cd058",
   "metadata": {},
   "source": [
    "### äº”ï¼Œè®¡ç®—å›¾åœ¨TensorBoardä¸­çš„å¯è§†åŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24de8b59",
   "metadata": {},
   "source": [
    "å¯ä»¥åˆ©ç”¨ torch.utils.tensorboard å°†è®¡ç®—å›¾å¯¼å‡ºåˆ° TensorBoardè¿›è¡Œå¯è§†åŒ–ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9497022f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.w = nn.Parameter(torch.randn(2,1))\n",
    "        self.b = nn.Parameter(torch.zeros(1,1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x@self.w + self.b\n",
    "        return y\n",
    "\n",
    "net = Net()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9df8e3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('./data/tensorboard')\n",
    "writer.add_graph(net,input_to_model = torch.rand(10,2))\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c16bda61",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "#%tensorboard --logdir ./data/tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9d00426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Known TensorBoard instances:\n",
      "  - port 6008: logdir ./data/tensorboard (started 0:02:00 ago; pid 17648)\n",
      "  - port 6006: logdir ./data/tensorboard (started 0:07:07 ago; pid 2728)\n"
     ]
    }
   ],
   "source": [
    "from tensorboard import notebook\n",
    "notebook.list() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7b92e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kill: 2728: No such process\n"
     ]
    }
   ],
   "source": [
    "# !kill 2728"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be2740bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6008 (pid 17648), started 0:01:38 ago. (Use '!kill 17648' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-4a6f188a424e617b\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-4a6f188a424e617b\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6008;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#åœ¨tensorboardä¸­æŸ¥çœ‹æ¨¡å‹\n",
    "notebook.start(\"--logdir ./data/tensorboard --port=6008\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a16975",
   "metadata": {},
   "source": [
    "![](./data/2-3-è®¡ç®—å›¾å¯è§†åŒ–.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3849e436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bd7f99f",
   "metadata": {},
   "source": [
    "**å¦‚æœæœ¬ä¹¦å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼Œæƒ³é¼“åŠ±ä¸€ä¸‹ä½œè€…ï¼Œè®°å¾—ç»™æœ¬é¡¹ç›®åŠ ä¸€é¢—æ˜Ÿæ˜Ÿstarâ­ï¸ï¼Œå¹¶åˆ†äº«ç»™ä½ çš„æœ‹å‹ä»¬å–”ğŸ˜Š!** \n",
    "\n",
    "å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·\"ç®—æ³•ç¾é£Ÿå±‹\"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚\n",
    "\n",
    "ä¹Ÿå¯ä»¥åœ¨å…¬ä¼—å·åå°å›å¤å…³é”®å­—ï¼š**åŠ ç¾¤**ï¼ŒåŠ å…¥è¯»è€…äº¤æµç¾¤å’Œå¤§å®¶è®¨è®ºã€‚\n",
    "\n",
    "![ç®—æ³•ç¾é£Ÿå±‹logo.png](https://tva1.sinaimg.cn/large/e6c9d24egy1h41m2zugguj20k00b9q46.jpg)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,md",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
