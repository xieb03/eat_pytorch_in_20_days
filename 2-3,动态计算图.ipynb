{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8077c48",
   "metadata": {},
   "source": [
    "# 2-3,动态计算图"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a500f1",
   "metadata": {},
   "source": [
    "本节我们将介绍 Pytorch的动态计算图。\n",
    "\n",
    "包括： \n",
    "\n",
    "* 动态计算图简介\n",
    "\n",
    "* 计算图中的Function\n",
    "\n",
    "* 计算图和反向传播\n",
    "\n",
    "* 叶子节点和非叶子节点\n",
    "\n",
    "* 计算图在TensorBoard中的可视化\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cefbb31",
   "metadata": {},
   "source": [
    "### 一，动态计算图简介"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e9b86b",
   "metadata": {},
   "source": [
    "![](./data/torch动态图.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70c6ecf",
   "metadata": {},
   "source": [
    "Pytorch的计算图由节点和边组成，节点表示张量或者Function，边表示张量和Function之间的依赖关系。\n",
    "\n",
    "Pytorch中的计算图是动态图。这里的动态主要有两重含义。\n",
    "\n",
    "**第一层含义是：计算图的正向传播是立即执行的。无需等待完整的计算图创建完毕，每条语句都会在计算图中动态添加节点和边，并立即执行正向传播得到计算结果。**\n",
    "\n",
    "**第二层含义是：计算图在反向传播后立即销毁。下次调用需要重新构建计算图。如果在程序中使用了backward方法执行了反向传播，或者利用torch.autograd.grad方法计算了梯度，那么创建的计算图会被立即销毁，释放存储空间，下次调用需要重新创建。这就是为什么上一章要用 retain_graph = True 的原因，算完一次之后，原有的计算图已经不存在了，相当于已经把公式忘记了**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5c20489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.__version__ =  2.0.1+cu118\n",
      "tensorboard.__version__ =  2.13.0\n",
      "torchvision.__version__ =  0.15.2+cu118\n",
      "pytorch_lightning.__version__ =  2.0.2\n",
      "torchtext.__version__ =  0.15.2\n",
      "torchdata.__version__ =  0.6.1\n",
      "torchmetrics.__version__ =  0.11.4\n",
      "torchkeras.__version__ =  3.8.2\n",
      "yaml.__version__ =  6.0\n",
      "tensorflow sed random seed fail.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import torchkeras\n",
    "\n",
    "#打印时间\n",
    "def printbar():\n",
    "    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(\"\\n\"+\"==========\"*8 + \"%s\"%nowtime)\n",
    "\n",
    "#mac系统上pytorch和matplotlib在jupyter中同时跑需要更改环境变量\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\" \n",
    "from python_cgtools.utils_date import *\n",
    "from python_cgtools.utils_torch import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9153578b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-29 12:32:04:start.........\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print_with_time(\"start.........\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1707038",
   "metadata": {},
   "source": [
    "**1，计算图的正向传播是立即执行的。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19d73e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.7748)\n",
      "tensor([[ 4.6815],\n",
      "        [ 7.5891],\n",
      "        [-4.4283],\n",
      "        [ 6.0147],\n",
      "        [-3.0943],\n",
      "        [ 5.8340],\n",
      "        [ 6.5043],\n",
      "        [ 5.6292],\n",
      "        [-3.0152],\n",
      "        [ 4.2290]])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "w = torch.tensor([[3.0,1.0]],requires_grad=True)\n",
    "b = torch.tensor([[3.0]],requires_grad=True)\n",
    "X = torch.randn(10,2)\n",
    "Y = torch.randn(10,1)\n",
    "Y_hat = X@w.t() + b  # Y_hat定义后其正向传播被立即执行，与其后面的loss创建语句无关\n",
    "loss = torch.mean(torch.pow(Y_hat-Y,2))\n",
    "\n",
    "print(loss.data)\n",
    "print(Y_hat.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f92804c",
   "metadata": {},
   "source": [
    "```\n",
    "tensor(17.8969)\n",
    "tensor([[3.2613],\n",
    "        [4.7322],\n",
    "        [4.5037],\n",
    "        [7.5899],\n",
    "        [7.0973],\n",
    "        [1.3287],\n",
    "        [6.1473],\n",
    "        [1.3492],\n",
    "        [1.3911],\n",
    "        [1.2150]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8fd5db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "adbd5f4b",
   "metadata": {},
   "source": [
    "**2，计算图在反向传播后立即销毁。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8506281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "w = torch.tensor([[3.0,1.0]],requires_grad=True)\n",
    "b = torch.tensor([[3.0]],requires_grad=True)\n",
    "X = torch.randn(10,2)\n",
    "Y = torch.randn(10,1)\n",
    "Y_hat = X@w.t() + b  # Y_hat定义后其正向传播被立即执行，与其后面的loss创建语句无关\n",
    "loss = torch.mean(torch.pow(Y_hat-Y,2))\n",
    "\n",
    "#计算图在反向传播后立即销毁，如果需要保留计算图, 需要设置retain_graph = True\n",
    "loss.backward()  #loss.backward(retain_graph = True) \n",
    "\n",
    "#loss.backward() #如果再次执行反向传播将报错\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5561cef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d2dbb7f",
   "metadata": {},
   "source": [
    "### 二，计算图中的Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7707accc",
   "metadata": {},
   "source": [
    "计算图中的 张量我们已经比较熟悉了, 计算图中的另外一种节点是Function, 实际上就是 Pytorch中各种对张量操作的函数。\n",
    "\n",
    "这些Function和我们Python中的函数有一个较大的区别，那就是它同时包括正向计算逻辑和反向传播的逻辑。\n",
    "\n",
    "我们可以通过继承torch.autograd.Function来创建这种支持反向传播的Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbc5a73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grad_output 是一直从最后反向传过来的导数\n",
    "# y = relu(x)\n",
    "# grad_output = dloss / dy\n",
    "# grad_input = dloss / dx = (dloss / dy) * (dy / dx) = grad_outpout * d relu(x) / dx\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "   \n",
    "    #正向传播逻辑，可以用ctx存储一些值，供反向传播使用。\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    #反向传播逻辑\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7624f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000]])\n",
      "tensor([[4.5000]])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "w = torch.tensor([[3.0,1.0]],requires_grad=True)\n",
    "b = torch.tensor([[3.0]],requires_grad=True)\n",
    "X = torch.tensor([[-1.0,-1.0],[1.0,1.0]])\n",
    "Y = torch.tensor([[2.0,3.0]])\n",
    "\n",
    "# relu现在也可以具有正向传播和反向传播功能\n",
    "relu = MyReLU.apply\n",
    "Y_hat = relu(X@w.t() + b)\n",
    "loss = torch.mean(torch.pow(Y_hat-Y,2))\n",
    "\n",
    "# dloss / dy_hat = 2(y_hat - y)\n",
    "# y_hat = 2(wx + b - y)\n",
    "# dloss / db = (dloss / dy_hat) * (dy_hat / dw) = 2(y_hat - y) * 2\n",
    "# dloss / dw = (dloss / dy_hat) * (dy_hat / dw) = 2(y_hat - y) * 2x\n",
    "loss.backward()\n",
    "\n",
    "# tensor([[4.5000, 4.5000]])\n",
    "# tensor([[4.5000]])\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d557de",
   "metadata": {},
   "source": [
    "```\n",
    "tensor([[4.5000, 4.5000]])\n",
    "tensor([[4.5000]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5570da3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.autograd.function.MyReLUBackward object at 0x00000227FC3D4900>\n"
     ]
    }
   ],
   "source": [
    "# Y_hat的梯度函数即是我们自己所定义的 MyReLU.backward\n",
    "# <torch.autograd.function.MyReLUBackward object at 0x000001EE7D663900>\n",
    "print(Y_hat.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbe47c6",
   "metadata": {},
   "source": [
    "```\n",
    "<torch.autograd.function.MyReLUBackward object at 0x1205a46c8>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01c3f73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-f48dd163d866>:3: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:491.)\n",
      "  print(Y_hat.grad)\n"
     ]
    }
   ],
   "source": [
    "# UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed\n",
    "# None\n",
    "print(Y_hat.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6753209e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.) tensor(-4.)\n"
     ]
    }
   ],
   "source": [
    "# 多元函数示范\n",
    "\n",
    "class MyMinus(torch.autograd.Function):\n",
    "   \n",
    "    #正向传播逻辑，可以用ctx存储一些值，供反向传播使用。\n",
    "    @staticmethod\n",
    "    def forward(ctx, a, b):\n",
    "        ctx.save_for_backward(a, b)\n",
    "        return a - b\n",
    "\n",
    "    #反向传播逻辑\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        a, b = ctx.saved_tensors\n",
    "        grad_a, grad_b = grad_output.clone(), -grad_output.clone()\n",
    "        return grad_a, grad_b\n",
    "    \n",
    "a = torch.tensor(3.0,requires_grad=True)\n",
    "b = torch.tensor(1.0,requires_grad=True)\n",
    "minus = MyMinus.apply\n",
    "loss = minus(a, b)**2\n",
    "loss.backward()\n",
    "# tensor(4.) tensor(-4.)\n",
    "print(a.grad, b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d70ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74879f4c",
   "metadata": {},
   "source": [
    "### 三，计算图与反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cd30c3",
   "metadata": {},
   "source": [
    "了解了Function的功能，我们可以简单地理解一下反向传播的原理和过程。理解该部分原理需要一些高等数学中求导链式法则的基础知识。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33988d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "x = torch.tensor(3.0,requires_grad=True)\n",
    "y1 = x + 1\n",
    "y2 = 2*x\n",
    "loss = (y1-y2)**2\n",
    "\n",
    "loss.backward()\n",
    "# tensor(4.)\n",
    "# dloss / dx = (dloss / dy1 * dy1 / dx) + (dloss / dy2 * dy2 / dx)\n",
    "#            = (2(y1 - y2) * 1) + (2(y2 - y1) * 2)\n",
    "#            = 2 * -2 * 1 + 2 * 2 * 2\n",
    "#            = -4 + 8\n",
    "#            = 4\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1fc3d8",
   "metadata": {},
   "source": [
    "loss.backward()语句调用后，依次发生以下计算过程。\n",
    "\n",
    "1，loss自己的grad梯度赋值为1，即对自身的梯度为1。\n",
    "\n",
    "2，loss根据其自身梯度以及关联的backward方法，计算出其对应的自变量即y1和y2的梯度，将该值赋值到y1.grad和y2.grad。\n",
    "\n",
    "**3，y2和y1根据其自身梯度以及关联的backward方法, 分别计算出其对应的自变量x的梯度，x.grad将其收到的多个梯度值累加。**\n",
    "\n",
    "（注意，1,2,3步骤的求梯度顺序和对多个梯度值的累加规则恰好是求导链式法则的程序表述）\n",
    "\n",
    "正因为求导链式法则衍生的梯度累加规则，张量的grad梯度不会自动清零，在需要的时候需要手动置零。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d86676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c20cd66d",
   "metadata": {},
   "source": [
    "### 四，叶子节点和非叶子节点"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db4c2fc",
   "metadata": {},
   "source": [
    "执行下面代码，我们会发现 loss.grad并不是我们期望的1,而是 None。\n",
    "\n",
    "类似地 y1.grad 以及 y2.grad也是 None.\n",
    "\n",
    "这是为什么呢？这是由于它们不是叶子节点张量。\n",
    "\n",
    "**在反向传播过程中，只有 is_leaf=True 的叶子节点，需要求导的张量的导数结果才会被最后保留下来。\n",
    "那么什么是叶子节点张量呢？叶子节点张量需要满足两个条件。\n",
    "1，叶子节点张量是由用户直接创建的张量，而非由某个Function通过计算得到的张量。\n",
    "2，叶子节点张量的 requires_grad属性必须为True.**\n",
    "\n",
    "Pytorch设计这样的规则主要是为了节约内存或者显存空间，因为几乎所有的时候，用户只会关心他自己直接创建的张量的梯度。\n",
    "\n",
    "**所有依赖于叶子节点张量的张量, 其requires_grad 属性必定是True的，但其梯度值只在计算过程中被用到，不会最终存储到grad属性中。**\n",
    "\n",
    "如果需要保留中间计算结果的梯度到grad属性中，可以使用 retain_grad方法。\n",
    "如果仅仅是为了调试代码查看梯度值，可以利用register_hook打印日志。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fb81bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.grad: None\n",
      "y1.grad: None\n",
      "y2.grad: None\n",
      "tensor(4.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-77fbc144ec27>:13: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:491.)\n",
      "  print(\"loss.grad:\", loss.grad)\n",
      "<ipython-input-11-77fbc144ec27>:14: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:491.)\n",
      "  print(\"y1.grad:\", y1.grad)\n",
      "<ipython-input-11-77fbc144ec27>:15: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:491.)\n",
      "  print(\"y2.grad:\", y2.grad)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "x = torch.tensor(3.0,requires_grad=True)\n",
    "y1 = x + 1\n",
    "y2 = 2*x\n",
    "loss = (y1-y2)**2\n",
    "\n",
    "loss.backward()\n",
    "# loss.grad: None\n",
    "# y1.grad: None\n",
    "# y2.grad: None\n",
    "# tensor(4.)\n",
    "print(\"loss.grad:\", loss.grad)\n",
    "print(\"y1.grad:\", y1.grad)\n",
    "print(\"y2.grad:\", y2.grad)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1542934b",
   "metadata": {},
   "source": [
    "```\n",
    "loss.grad: None\n",
    "y1.grad: None\n",
    "y2.grad: None\n",
    "tensor(4.)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e253648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# True\n",
    "# False\n",
    "# False\n",
    "# False\n",
    "print(x.is_leaf)\n",
    "print(y1.is_leaf)\n",
    "print(y2.is_leaf)\n",
    "print(loss.is_leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7670308b",
   "metadata": {},
   "source": [
    "```\n",
    "True\n",
    "False\n",
    "False\n",
    "False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207753e2",
   "metadata": {},
   "source": [
    "利用retain_grad可以保留非叶子节点的梯度值，利用register_hook可以查看非叶子节点的梯度值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22d9401c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y2 grad:  tensor(4.)\n",
      "y1 grad:  tensor(-4.)\n",
      "11111\n",
      "loss.grad: tensor(1.)\n",
      "y.grad: None\n",
      "x.grad: tensor(4.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-7936a861ceb3>:26: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:491.)\n",
      "  print(\"y.grad:\", y1.grad)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "#正向传播\n",
    "x = torch.tensor(3.0,requires_grad=True)\n",
    "y1 = x + 1\n",
    "y2 = 2*x\n",
    "loss = (y1-y2)**2\n",
    "\n",
    "#非叶子节点梯度显示控制\n",
    "y1.register_hook(lambda grad: print('y1 grad: ', grad))\n",
    "y2.register_hook(lambda grad: print('y2 grad: ', grad))\n",
    "loss.retain_grad()\n",
    "\n",
    "#反向传播\n",
    "loss.backward()\n",
    "\n",
    "# y2 grad:  tensor(4.)\n",
    "# y1 grad:  tensor(-4.)\n",
    "# 注意上述代码是在 loss.backward() 打印出来的\n",
    "# 11111\n",
    "# loss.grad: tensor(1.)\n",
    "# y.grad: None\n",
    "# x.grad: tensor(4.)\n",
    "print(11111)\n",
    "print(\"loss.grad:\", loss.grad)\n",
    "print(\"y.grad:\", y1.grad)\n",
    "print(\"x.grad:\", x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91887ca8",
   "metadata": {},
   "source": [
    "```\n",
    "y2 grad:  tensor(4.)\n",
    "y1 grad:  tensor(-4.)\n",
    "loss.grad: tensor(1.)\n",
    "x.grad: tensor(4.)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3110a64c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e11cd058",
   "metadata": {},
   "source": [
    "### 五，计算图在TensorBoard中的可视化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24de8b59",
   "metadata": {},
   "source": [
    "可以利用 torch.utils.tensorboard 将计算图导出到 TensorBoard进行可视化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9497022f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.w = nn.Parameter(torch.randn(2,1))\n",
    "        self.b = nn.Parameter(torch.zeros(1,1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x@self.w + self.b\n",
    "        return y\n",
    "\n",
    "net = Net()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9df8e3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('./data/tensorboard')\n",
    "writer.add_graph(net,input_to_model = torch.rand(10,2))\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c16bda61",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "#%tensorboard --logdir ./data/tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9d00426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Known TensorBoard instances:\n",
      "  - port 6008: logdir ./data/tensorboard (started 0:02:00 ago; pid 17648)\n",
      "  - port 6006: logdir ./data/tensorboard (started 0:07:07 ago; pid 2728)\n"
     ]
    }
   ],
   "source": [
    "from tensorboard import notebook\n",
    "notebook.list() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7b92e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kill: 2728: No such process\n"
     ]
    }
   ],
   "source": [
    "# !kill 2728"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be2740bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6008 (pid 17648), started 0:01:38 ago. (Use '!kill 17648' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-4a6f188a424e617b\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-4a6f188a424e617b\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6008;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#在tensorboard中查看模型\n",
    "notebook.start(\"--logdir ./data/tensorboard --port=6008\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a16975",
   "metadata": {},
   "source": [
    "![](./data/2-3-计算图可视化.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3849e436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bd7f99f",
   "metadata": {},
   "source": [
    "**如果本书对你有所帮助，想鼓励一下作者，记得给本项目加一颗星星star⭐️，并分享给你的朋友们喔😊!** \n",
    "\n",
    "如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号\"算法美食屋\"下留言。作者时间和精力有限，会酌情予以回复。\n",
    "\n",
    "也可以在公众号后台回复关键字：**加群**，加入读者交流群和大家讨论。\n",
    "\n",
    "![算法美食屋logo.png](https://tva1.sinaimg.cn/large/e6c9d24egy1h41m2zugguj20k00b9q46.jpg)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,md",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
